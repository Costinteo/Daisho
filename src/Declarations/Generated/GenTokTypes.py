#!/bin/python

# [TokType, literal]


def pcase(s): return [s.upper(), s.lower()]

exact_tokens = [
    # Pragma
    pcase("native"),
    pcase("ctype"),

    # Types
    ["BOOL", "Bool"],
    ["CHAR", "Char"],
    ["UCHAR", "UChar"],
    ["SHORT", "Short"],
    ["USHORT", "UShort"],
    ["INT", "Int"],
    ["UINT", "UInt"],
    ["LONG", "Long"],
    ["FLOAT", "Float"],
    ["DOUBLE", "Double"],
    ["VOID", "Void"],

    # Control
    pcase('if'),
    pcase('else'),

    # Loops
    pcase('for'),
    pcase('while'),
    pcase('continue'),
    pcase('break'),
    pcase('in'),

    # Exceptions
    # makeLower('try'),
    # makeLower('catch'),
    # makeLower('finally'),

    # Classes
    pcase('class'),
    pcase('this'),
    pcase('operator'),
    # pcase('extends'),

    # Interfaces
    pcase('trait'),
    pcase('impl'),

    # Other containers
    pcase('enum'),

    # Access Modifiers
    pcase('private'),
    pcase('protected'),
    pcase('public'),

    # Builtin functions
    pcase('instanceof'),
    pcase('sizeof'),
    pcase('assert'),

    # Boolean literals
    pcase('true'),
    pcase('false'),

    # Separators
    ["LPAREN", "("],
    ["RPAREN", ")"],
    ["LBRACE", "{"],
    ["RBRACE", "}"],
    ["LBRACK", "["],
    ["RBRACK", "]"],
    ["LARROW", "<"],
    ["RARROW", ">"],
    ["SEMI", ";"],
    ["COMMA", ","],
    ["DOT", "."],
    ["STAR", "*"],
    ["EQUALS", "="],
    ["LAMBDA_ARROW", "=>"],

    # Operators
    ["BANG", "!"],
    ["TILDE", "~"],
    ["QUESTION", "?"],
    ["COLON", ":"],
    ["EQUAL", "=="],
    ["LE", "<="],
    ["GE", ">="],
    ["NOTEQUAL", "!="],
    ["AND", "&&"],
    ["OR", "||"],
    ["INC", "++"],
    ["DEC", "--"],
    ["ADD", "+"],
    ["SUB", "-"],
    ["DIV", "/"],
    ["AMP", "&"],
    ["BITOR", "|"],
    ["CARET", "^"],
    ["MOD", "%"],
    ["DEREF_ARROW", "->"],

    ["ADD_ASSIGN", "+="],
    ["SUB_ASSIGN", "-="],
    ["MUL_ASSIGN", "*="],
    ["DIV_ASSIGN", "/="],
    ["AND_ASSIGN", "&="],
    ["OR_ASSIGN", "|="],
    ["XOR_ASSIGN", "^="],
    ["MOD_ASSIGN", "%="],
]

custom = ['WS', 'IMPORT', 'SL_COMMENT', 'ML_COMMENT', 'IDENT', 'END_OF_FILE', 'INVALID']

names =  [k[0] for k in exact_tokens] + custom

automata_description = """
/*
 * This file (Generated by GenTokTypes.py, edits will be overwritten) contains a set of 
 * rules for Stilts's NFA-based tokenizer.
 *
 * Every type of Stilts token is represented by a single deterministic finite automaton.
 * Each automaton reads one character of the imput at a time. If the character matches a 
 * transition rule (start_range and end_range are both inclusive) and the DFA is currently 
 * in the matching start_state, then it will change state to the corresponding end_state. 
 *
 * There may be multiple transition rules that match. Ties are broken by taking the first 
 * applicable transition. If no transition rule applies, the DFA transitions into the 0 
 * state, which is non-accepting, and no further input will be processed.
 *
 * Tokenization continues until one of the following:
 *   One or less of the DFAs are accepting
 *   The end of the file is reached.
 * At this point, we rewind to the last position where a DFA was accepting, and break any 
 * ties. We take that token, add it to the token stream, advance our position in the file, 
 * and continue for the next token. If there's no token to add because we're at the end of 
 * the file, or some type of syntax error occurred where there is no valid token to add, 
 * we stop processing and either end the token stream with the END_OF_FILE token, or throw 
 * an error.
 */
"""

custom_rules = """
/* IDENT */
/* First char of ident, [_a-zA-Zalpha-omegaALPHA-OMEGA] */
#define IDENT_rule_1  { .start_range = 'a', .end_range = 'z', .start_state = 1, .end_state = 2 }
#define IDENT_rule_2  { .start_range = 'A', .end_range = 'Z', .start_state = 1, .end_state = 2 }
#define IDENT_rule_3  { .start_range = '_', .end_range = '_', .start_state = 1, .end_state = 2 }
#define IDENT_rule_4  { .start_range = 0xceb1, .end_range = 0xcf89, .start_state = 1, .end_state = 2 }
#define IDENT_rule_5  { .start_range = 0xcea9, .end_range = 0xce91, .start_state = 1, .end_state = 2 }
/* Remaining chars of ident [_a-zA-Zalpha-omegaALPHA-OMEGA0-9] */
#define IDENT_rule_6  { .start_range = 'a', .end_range = 'z', .start_state = 2, .end_state = 2 }
#define IDENT_rule_7  { .start_range = 'A', .end_range = 'Z', .start_state = 2, .end_state = 2 }
#define IDENT_rule_8  { .start_range = '_', .end_range = '_', .start_state = 2, .end_state = 2 }
#define IDENT_rule_9  { .start_range = 0xceb1, .end_range = 0xcf89, .start_state = 2, .end_state = 2 }
#define IDENT_rule_10 { .start_range = 0xcea9, .end_range = 0xce91, .start_state = 2, .end_state = 2 }
#define IDENT_rule_11 { .start_range = '0', .end_range = '9', .start_state = 2, .end_state = 2 }
#define IDENT_rules   { IDENT_rule_1, IDENT_rule_2, IDENT_rule_3, IDENT_rule_4, IDENT_rule_5, IDENT_rule_6, IDENT_rule_7, IDENT_rule_8, IDENT_rule_9, IDENT_rule_10, IDENT_rule_11 }
#define IDENT_DFA     { .rules = IDENT_rules, .num_rules = ARR_SIZE(IDENT_rules), .accepting_state = 2 }

/* Whitespace (Any amount) */
/* Start rule */
#define WS_rule_1 { .start_range = ' ', .end_range = ' ', .start_state = 1, .end_state = 1 }
#define WS_rule_2 { .start_range = '\\t', .end_range = '\\t', .start_state = 1, .end_state = 1 }
#define WS_rule_3 { .start_range = '\\r', .end_range = '\\r', .start_state = 1, .end_state = 1 }
#define WS_rule_4 { .start_range = '\\n', .end_range = '\\n', .start_state = 1, .end_state = 1 }
#define WS_rules  { WS_rule_1, WS_rule_2, WS_rule_3, WS_rule_4 }
#define WS_DFA    { .rules = WS_rules, .num_rules = ARR_SIZE(WS_rules), .accepting_state = 2 }

/* Multi-Line Comment */
#define ML_COMMENT_rule_1 { .start_range = '/', .end_range = '/', .start_state = 1, .end_state = 2 }
#define ML_COMMENT_rule_2 { .start_range = '*', .end_range = '*', .start_state = 2, .end_state = 3 }
#define ML_COMMENT_rule_3 { .start_range = '*', .end_range = '*', .start_state = 3, .end_state = 4 }
#define ML_COMMENT_rule_4 { .start_range = '/', .end_range = '/', .start_state = 4, .end_state = 5 }
// Capture all but */ while inside comment
#define ML_COMMENT_rule_5 { .start_range = 0, .end_range = NFASTATE_MAX, .start_state = 3, .end_state = 3 }
#define ML_COMMENT_rules  { ML_COMMENT_rule_1, ML_COMMENT_rule_2, ML_COMMENT_rule_3, ML_COMMENT_rule_4, ML_COMMENT_rule_5 }
#define ML_COMMENT_DFA    { .rules = ML_COMMENT_rules, .num_rules = ARR_SIZE(ML_COMMENT_rules), .accepting_state = 5 }

/* Single-Line Comment */
#define SL_COMMENT_rule_1 { .start_range = '/', .end_range = '/', .start_state = 1, .end_state = 2 }
#define SL_COMMENT_rule_2 { .start_range = '/', .end_range = '/', .start_state = 2, .end_state = 3 }
#define SL_COMMENT_rule_3 { .start_range = '\\n', .end_range = '\\n', .start_state = 3, .end_state = 4 }
// Capture all but \\n while inside comment
#define SL_COMMENT_rule_4 { .start_range = 0, .end_range = NFASTATE_MAX, .start_state = 3, .end_state = 3 }
#define SL_COMMENT_rules  { SL_COMMENT_rule_1, SL_COMMENT_rule_2, SL_COMMENT_rule_3, SL_COMMENT_rule_4 }
#define SL_COMMENT_DFA    { .rules = SL_COMMENT_rules, .num_rules = ARR_SIZE(SL_COMMENT_rules), .accepting_state = 4 }

/* Import / Include */
#define IMPORT_rule_1  { .start_range = 'i', .end_range = 'i', .start_state = 1, .end_state = 2 }
#define IMPORT_rule_2  { .start_range = 'm', .end_range = 'i', .start_state = 2, .end_state = 3 }
#define IMPORT_rule_3  { .start_range = 'p', .end_range = 'i', .start_state = 3, .end_state = 4 }
#define IMPORT_rule_4  { .start_range = 'o', .end_range = 'i', .start_state = 4, .end_state = 5 }
#define IMPORT_rule_5  { .start_range = 'r', .end_range = 'i', .start_state = 5, .end_state = 6 }
#define IMPORT_rule_6  { .start_range = 't', .end_range = 'i', .start_state = 6, .end_state = 8 }
#define IMPORT_rule_7  { .start_range = 'n', .end_range = 'i', .start_state = 2, .end_state = 3 }
#define IMPORT_rule_8  { .start_range = 'c', .end_range = 'i', .start_state = 3, .end_state = 4 }
#define IMPORT_rule_9  { .start_range = 'l', .end_range = 'i', .start_state = 4, .end_state = 5 }
#define IMPORT_rule_10 { .start_range = 'u', .end_range = 'i', .start_state = 5, .end_state = 6 }
#define IMPORT_rule_11 { .start_range = 'd', .end_range = 'i', .start_state = 6, .end_state = 7 }
#define IMPORT_rule_12 { .start_range = 'e', .end_range = 'i', .start_state = 7, .end_state = 8 }
#define IMPORT_rules   { IMPORT_rule_1, IMPORT_rule_2, IMPORT_rule_3, IMPORT_rule_4, IMPORT_rule_5, IMPORT_rule_6, IMPORT_rule_7, IMPORT_rule_8, IMPORT_rule_9, IMPORT_rule_10, IMPORT_rule_11, IMPORT_rule_12 }
#define IMPORT_DFA     { .rules = IMPORT_rules, .num_rules = ARR_SIZE(IMPORT_rules), .accepting_state = 8 }


"""

def flowerbox(str):
    tb = "/" + ("*"*(len(str)+4)) + "/\n"
    f.write(tb)
    f.write("/* " + str + " */\n")
    f.write(tb)


def writeValid(name, literal):
    f.write(
        f"static inline TokType valid_{name}(char* str) {{ static const char* tok = \"{literal}\"; const char *s = tok; while (*s) ++s; size_t toklen = (size_t)(s - str); for (size_t i = 0; i < toklen; i++) if (str[i] != tok[i]) return INVALID; return {name}; }}\n")


def writePotential(name, literal):
    f.write(
        f"static inline bool potential_{name}(char* str) {{ static const char* tok = \"{literal}\"; while (*str) if (*str++ != *tok++) return false; return true; }}\n")


def writeExactDFARules(name, literal):

    for n, c in enumerate(literal):
        f.write(
            f'#define {name}_rule_{n+1} {{ .start_range = \'{c}\', .end_range = \'{c}\', .start_state = {n+1}, .end_state = {n+2} }};\n')
    f.write(
        f'#define {name}_rules {{ {", ".join([name + "_rule_" + str(n+1) for n in range(len(literal)) ])} }};\n')
    f.write(
        f'#define {name}_DFA {{ .rules = {name}_rules, .num_rules = ARR_SIZE({name}_rules), .accepting_state = {len(literal)+1} }};\n\n')


with open("TokType.h", 'w') as f:
    # Header
    f.write('// THIS FILE GENERATED BY GenTokType.py. DO NOT EDIT.\n')
    f.write('#ifndef INCLUDE_TOKENS\n')
    f.write('#define INCLUDE_TOKENS\n')
    f.write('#include <apaz-libc.h>\n\n')

    # Declare token types
    f.write(f'#define NUM_TOKTYPES {len(names)}\n')
    f.write(f'#define MAX_TOKTYPE_NAME_LEN {max([len(n) for n in names])}\n')
    f.write('enum TokType {\n')
    [f.write(f"  {c},\n") for c in custom]
    [f.write(f"  {e[0]},\n") for e in exact_tokens]
    f.write('};\ntypedef enum TokType TokType;\n')

    # Declare reverse map from TokType to TOKNAME as a static array of string.
    dq = '"'
    nl = '\n'
    f.write(
        f'static const char* TokNameMap[] = {{{nl}{f", {nl}".join([f"  {dq}{name}{dq}" for name in names])}\n}};{nl}{nl}')

    # Write methods for tokenizaton that can be generated

    # Write the ones that can't be automatically generated.
    # f.write(custom_functions)

    # Footer
    f.write("\n#endif // INCLUDE_TOKENS")

with open('Automata.h', 'w') as f:
    f.write('// THIS FILE GENERATED BY GenTokType.py. DO NOT EDIT.\n')
    f.write('#ifndef INCLUDE_AUTOMATA\n')
    f.write('#define INCLUDE_AUTOMATA\n')
    f.write('#include <apaz-libc.h>\n')
    f.write('#include "../../UTF-8.h"\n\n')
    
    f.write(automata_description + "\n\n")

    f.write("typedef uint8_t NFAState;\n#define NFASTATE_MAX UINT8_MAX\n\nTYPE_DECLARE(DFARule);\nstruct DFARule {\n  // Start and end are inclusive\n  utf8_t start_range; utf8_t end_range;\n  NFAState start_state;\n  NFAState end_state;\n};\n\nTYPE_DECLARE(DFA);\nstruct DFA {\n  const DFARule* const rules;\n  size_t num_rules;\n  NFAState accepting_state;\n};\n\n")

    flowerbox("Exact Rules")
    for e in exact_tokens:
        writeExactDFARules(e[0], e[1])
    f.write('\n')
    flowerbox("Custom Rules")
    f.write(custom_rules + "\n\n")

    flowerbox("All DFAs")
    f.write(f'const DFA all_DFAs[] = {{ {", ".join([f"{n}_DFA" for n in [e[0] for e in exact_tokens] + custom[:len(custom)-2]])} }};\n\n')
    # f.write("" + ", ".join(custom[2:]) + )

    f.write('#endif // INCLUDE_AUTOMATA\n')
